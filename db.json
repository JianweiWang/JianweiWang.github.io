{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[],"Cache":[{"_id":"source/_posts/Network-Model-in-Kubernetes.md","hash":"1ac3d8ffac1251dfe6f4fa151f395f34a58028c5","modified":1527507533310},{"_id":"source/_posts/DNS-Cache-In-Linux.md","hash":"13cf449960b779d7bf50c27f537be683d5281c26","modified":1527507723117},{"_id":"source/_posts/Service-And-Kube-proxy.md","hash":"0ca5145788e9c15d530afca417bb0b7577d26c6f","modified":1527676115951},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-02d62faa.png","hash":"79a58615dcc1f621de66fb31206a569abe483ba9","modified":1527664470092},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-4afab240.png","hash":"491a0f6865ddbeab519c405758282ae65948317c","modified":1527663202131},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-9a3a490b.png","hash":"6285b2407939dc17a2eb5a0aa5aebe2eda57457e","modified":1527662122441},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-90b65a4a.png","hash":"c75a95ceba88cc7a5b68698c2a403b2475c2ecc0","modified":1527664241954},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-9bf039d8.png","hash":"3f20b732fa711033c859624c66787e762f2f8782","modified":1527664285503},{"_id":"source/_posts/assets/Service-And-Kube-proxy-a219618a.png","hash":"a039f95572d93a7a14956da19c4ffb72c3f6ce22","modified":1527675547883},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-323364fe.png","hash":"81ab8ca6d1d1ed2f1104998b5ea7f8c8cc9eb4e0","modified":1527675671071},{"_id":"source/_posts/Network-Model-in-Kubernetes/Network-Model-in-Kubernetes-6c165903.png","hash":"671aefe0b91342cfcadc87b64545fdc41ba08427","modified":1527507533311},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-6cc5b0ce.png","hash":"7cb37151a236268fa1484713eb8a7829363f5748","modified":1527670473558},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-d314f07b.png","hash":"0703aefb7e3dd48c48dfad6d4a728f13b47c4e09","modified":1527670490264},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-74a4dc02.png","hash":"c1302b1886affaf6566687a1ddb5a03e7d9d10bb","modified":1527664138997},{"_id":"public/2018/05/27/Service-And-Kube-proxy/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527676133448},{"_id":"public/2018/05/27/Network-Model-in-Kubernetes/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527676133448},{"_id":"public/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527676133449},{"_id":"public/archives/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527676133449},{"_id":"public/archives/2018/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527676133449},{"_id":"public/archives/2018/05/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527676133449},{"_id":"public/tags/kubernetes-Network/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527676133449},{"_id":"public/tags/service/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527676133449},{"_id":"public/2018/05/28/DNS-Cache-In-Linux/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527676133452},{"_id":"public/2018/05/27/Service-And-Kube-proxy/Service-And-Kube-proxy-02d62faa.png","hash":"79a58615dcc1f621de66fb31206a569abe483ba9","modified":1527676133454},{"_id":"public/2018/05/27/Service-And-Kube-proxy/Service-And-Kube-proxy-4afab240.png","hash":"491a0f6865ddbeab519c405758282ae65948317c","modified":1527676133454},{"_id":"public/2018/05/27/Service-And-Kube-proxy/Service-And-Kube-proxy-90b65a4a.png","hash":"c75a95ceba88cc7a5b68698c2a403b2475c2ecc0","modified":1527676133454},{"_id":"public/2018/05/27/Service-And-Kube-proxy/Service-And-Kube-proxy-9a3a490b.png","hash":"6285b2407939dc17a2eb5a0aa5aebe2eda57457e","modified":1527676133454},{"_id":"public/2018/05/27/Service-And-Kube-proxy/Service-And-Kube-proxy-9bf039d8.png","hash":"3f20b732fa711033c859624c66787e762f2f8782","modified":1527676133454},{"_id":"public/2018/05/27/Network-Model-in-Kubernetes/Network-Model-in-Kubernetes-6c165903.png","hash":"671aefe0b91342cfcadc87b64545fdc41ba08427","modified":1527676133456},{"_id":"public/2018/05/27/Service-And-Kube-proxy/Service-And-Kube-proxy-323364fe.png","hash":"81ab8ca6d1d1ed2f1104998b5ea7f8c8cc9eb4e0","modified":1527676133456},{"_id":"public/2018/05/27/Service-And-Kube-proxy/Service-And-Kube-proxy-6cc5b0ce.png","hash":"7cb37151a236268fa1484713eb8a7829363f5748","modified":1527676133456},{"_id":"public/2018/05/27/Service-And-Kube-proxy/Service-And-Kube-proxy-d314f07b.png","hash":"0703aefb7e3dd48c48dfad6d4a728f13b47c4e09","modified":1527676133456},{"_id":"public/2018/05/27/Service-And-Kube-proxy/Service-And-Kube-proxy-74a4dc02.png","hash":"c1302b1886affaf6566687a1ddb5a03e7d9d10bb","modified":1527676133457}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"Network Model in Kubernetes","date":"2018-05-27T11:00:45.000Z","_content":"本文主要剖析kubernetes中网络模型，详细介绍pod之间的通信流程。\n\n## 网络模型概况\n### kubernetes对网络的要求\n* 所有的容器都可以在不用NAT的方式下同别的容器通信\n* 所有容器节点都可以在不用NAT的方式下同所有容器通信\n* 容器的地址和别人看到的地址是同一个地址\n\n### kubernetes的网络场景\n* 容器间直接通信\n\n  同一个pod中，不通容器共享一个网络命名空间，共享同一个Linux协议栈，可以直接通过localhost通信。\n* 同一个Node上pod间的通信\n\n  同一个Node内，不同的Pod都有一个全局IP，可以直接通过Pod的IP进行通信。Pod地址和docker0在同一个网段\n\n* 不同Node上的pod间通信\n\n  docker0网桥与宿主机网卡是两个完全不同的IP网段，并且Node之间的通信只能通过宿主机的物理网卡进行，因此要想实现位于不同Node上的Pod容器之间的通信，就必须想办法通过主机的IP地址进行寻址和通信。Kubernetes会记录所有正在运行的Pod的IP分配信息，并且将这些信息保存在etcd中（作为service的Endpoint）。\n\n  因此，实现不同Node上pod之间的通信，需要两个条件\n    * pod IP做整体规划，整个kubernetes集群内的pod IP不能有冲突，一般通过第三方开源工具管理，如flannel\n    * 将Node IP与该Node内的pod IP关联起来，通过Node IP转发到pod IP\n\n* pod到service的通信\n\n  pod到service的通信通过kube-proxy实现，底层通过修改iptables规则实现，后文具体介绍。\n\n* 集群外部与内部组件的通信\n\n  通过NodePort或者Ingress实现；\n\n### kubernetes网络结构\n为了满足上述要求和场景，kubernetes集群的网络分为三层：\n\n* 集群物理网络，即Node节点所属网络；\n* Node内部的容器网络，一般是docker0网卡负责管理；\n* Node网络和容器网络的连接器，overlay网络，一般flannel负责管理；\n\n三层网络的结构下图：\n\n![](Network-Model-in-Kubernetes-6c165903.png)\n\n<center>kubernetes网络结构</center>\n<br/>\n<br/>\n\n其中，docker0是Node内部不通pod间的默认路由，pod的IP地址从docker0动态分配；flannel负责overylay网络维护，把docker0内的私网和Node节点的网络打通；GateWay则是Node网络中的默认路由，负责物理网络数据包转发。\n\n### 开源网络组件Flannel\n在kubernetes中，flannel的左右有以下两点：\n* 给每个Node上的Docker容器分配不冲突的IP地址\n* 在这些IP之间建立一个覆盖网络(Overlay Network),通过这个覆盖网络，将数据包原封不动的传递到目标容器内\n\n首先，flannel创建一个flannel0网桥，一端连接docker0网桥，另一端连接flanneld的服务进程。flanneld进程跟kubernetes集群中的etcd通信，管理网段资源，同时监控每个pod的实际地址，并在内存中建立一个pod节点的路由表。flanneld进程一端连接flannel0网桥，另一端连接物理网络。\n\n<br/>\n\n综上，以pod0和pod2的通信为例，数据的流转是这样的：\n\npod0 --> docker0 --> flannel0 --> flanneld --> Node0物理网卡 --> Node1物理网卡 --> flanneld --> flannel0 --> docker0 --> pod2\n","source":"_posts/Network-Model-in-Kubernetes.md","raw":"---\ntitle: Network Model in Kubernetes\ndate: 2018-05-27 19:00:45\ntags: kubernetes Network\n---\n本文主要剖析kubernetes中网络模型，详细介绍pod之间的通信流程。\n\n## 网络模型概况\n### kubernetes对网络的要求\n* 所有的容器都可以在不用NAT的方式下同别的容器通信\n* 所有容器节点都可以在不用NAT的方式下同所有容器通信\n* 容器的地址和别人看到的地址是同一个地址\n\n### kubernetes的网络场景\n* 容器间直接通信\n\n  同一个pod中，不通容器共享一个网络命名空间，共享同一个Linux协议栈，可以直接通过localhost通信。\n* 同一个Node上pod间的通信\n\n  同一个Node内，不同的Pod都有一个全局IP，可以直接通过Pod的IP进行通信。Pod地址和docker0在同一个网段\n\n* 不同Node上的pod间通信\n\n  docker0网桥与宿主机网卡是两个完全不同的IP网段，并且Node之间的通信只能通过宿主机的物理网卡进行，因此要想实现位于不同Node上的Pod容器之间的通信，就必须想办法通过主机的IP地址进行寻址和通信。Kubernetes会记录所有正在运行的Pod的IP分配信息，并且将这些信息保存在etcd中（作为service的Endpoint）。\n\n  因此，实现不同Node上pod之间的通信，需要两个条件\n    * pod IP做整体规划，整个kubernetes集群内的pod IP不能有冲突，一般通过第三方开源工具管理，如flannel\n    * 将Node IP与该Node内的pod IP关联起来，通过Node IP转发到pod IP\n\n* pod到service的通信\n\n  pod到service的通信通过kube-proxy实现，底层通过修改iptables规则实现，后文具体介绍。\n\n* 集群外部与内部组件的通信\n\n  通过NodePort或者Ingress实现；\n\n### kubernetes网络结构\n为了满足上述要求和场景，kubernetes集群的网络分为三层：\n\n* 集群物理网络，即Node节点所属网络；\n* Node内部的容器网络，一般是docker0网卡负责管理；\n* Node网络和容器网络的连接器，overlay网络，一般flannel负责管理；\n\n三层网络的结构下图：\n\n![](Network-Model-in-Kubernetes-6c165903.png)\n\n<center>kubernetes网络结构</center>\n<br/>\n<br/>\n\n其中，docker0是Node内部不通pod间的默认路由，pod的IP地址从docker0动态分配；flannel负责overylay网络维护，把docker0内的私网和Node节点的网络打通；GateWay则是Node网络中的默认路由，负责物理网络数据包转发。\n\n### 开源网络组件Flannel\n在kubernetes中，flannel的左右有以下两点：\n* 给每个Node上的Docker容器分配不冲突的IP地址\n* 在这些IP之间建立一个覆盖网络(Overlay Network),通过这个覆盖网络，将数据包原封不动的传递到目标容器内\n\n首先，flannel创建一个flannel0网桥，一端连接docker0网桥，另一端连接flanneld的服务进程。flanneld进程跟kubernetes集群中的etcd通信，管理网段资源，同时监控每个pod的实际地址，并在内存中建立一个pod节点的路由表。flanneld进程一端连接flannel0网桥，另一端连接物理网络。\n\n<br/>\n\n综上，以pod0和pod2的通信为例，数据的流转是这样的：\n\npod0 --> docker0 --> flannel0 --> flanneld --> Node0物理网卡 --> Node1物理网卡 --> flanneld --> flannel0 --> docker0 --> pod2\n","slug":"Network-Model-in-Kubernetes","published":1,"updated":"2018-05-28T11:38:53.310Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhsyxci40000a78jrnbsbq6l","content":"<p>本文主要剖析kubernetes中网络模型，详细介绍pod之间的通信流程。</p>\n<h2 id=\"网络模型概况\"><a href=\"#网络模型概况\" class=\"headerlink\" title=\"网络模型概况\"></a>网络模型概况</h2><h3 id=\"kubernetes对网络的要求\"><a href=\"#kubernetes对网络的要求\" class=\"headerlink\" title=\"kubernetes对网络的要求\"></a>kubernetes对网络的要求</h3><ul>\n<li>所有的容器都可以在不用NAT的方式下同别的容器通信</li>\n<li>所有容器节点都可以在不用NAT的方式下同所有容器通信</li>\n<li>容器的地址和别人看到的地址是同一个地址</li>\n</ul>\n<h3 id=\"kubernetes的网络场景\"><a href=\"#kubernetes的网络场景\" class=\"headerlink\" title=\"kubernetes的网络场景\"></a>kubernetes的网络场景</h3><ul>\n<li><p>容器间直接通信</p>\n<p>同一个pod中，不通容器共享一个网络命名空间，共享同一个Linux协议栈，可以直接通过localhost通信。</p>\n</li>\n<li><p>同一个Node上pod间的通信</p>\n<p>同一个Node内，不同的Pod都有一个全局IP，可以直接通过Pod的IP进行通信。Pod地址和docker0在同一个网段</p>\n</li>\n<li><p>不同Node上的pod间通信</p>\n<p>docker0网桥与宿主机网卡是两个完全不同的IP网段，并且Node之间的通信只能通过宿主机的物理网卡进行，因此要想实现位于不同Node上的Pod容器之间的通信，就必须想办法通过主机的IP地址进行寻址和通信。Kubernetes会记录所有正在运行的Pod的IP分配信息，并且将这些信息保存在etcd中（作为service的Endpoint）。</p>\n<p>因此，实现不同Node上pod之间的通信，需要两个条件</p>\n<ul>\n<li>pod IP做整体规划，整个kubernetes集群内的pod IP不能有冲突，一般通过第三方开源工具管理，如flannel</li>\n<li>将Node IP与该Node内的pod IP关联起来，通过Node IP转发到pod IP</li>\n</ul>\n</li>\n<li><p>pod到service的通信</p>\n<p>pod到service的通信通过kube-proxy实现，底层通过修改iptables规则实现，后文具体介绍。</p>\n</li>\n<li><p>集群外部与内部组件的通信</p>\n<p>通过NodePort或者Ingress实现；</p>\n</li>\n</ul>\n<h3 id=\"kubernetes网络结构\"><a href=\"#kubernetes网络结构\" class=\"headerlink\" title=\"kubernetes网络结构\"></a>kubernetes网络结构</h3><p>为了满足上述要求和场景，kubernetes集群的网络分为三层：</p>\n<ul>\n<li>集群物理网络，即Node节点所属网络；</li>\n<li>Node内部的容器网络，一般是docker0网卡负责管理；</li>\n<li>Node网络和容器网络的连接器，overlay网络，一般flannel负责管理；</li>\n</ul>\n<p>三层网络的结构下图：</p>\n<p><img src=\"Network-Model-in-Kubernetes-6c165903.png\" alt=\"\"></p>\n<p><center>kubernetes网络结构</center><br><br><br><br></p>\n<p>其中，docker0是Node内部不通pod间的默认路由，pod的IP地址从docker0动态分配；flannel负责overylay网络维护，把docker0内的私网和Node节点的网络打通；GateWay则是Node网络中的默认路由，负责物理网络数据包转发。</p>\n<h3 id=\"开源网络组件Flannel\"><a href=\"#开源网络组件Flannel\" class=\"headerlink\" title=\"开源网络组件Flannel\"></a>开源网络组件Flannel</h3><p>在kubernetes中，flannel的左右有以下两点：</p>\n<ul>\n<li>给每个Node上的Docker容器分配不冲突的IP地址</li>\n<li>在这些IP之间建立一个覆盖网络(Overlay Network),通过这个覆盖网络，将数据包原封不动的传递到目标容器内</li>\n</ul>\n<p>首先，flannel创建一个flannel0网桥，一端连接docker0网桥，另一端连接flanneld的服务进程。flanneld进程跟kubernetes集群中的etcd通信，管理网段资源，同时监控每个pod的实际地址，并在内存中建立一个pod节点的路由表。flanneld进程一端连接flannel0网桥，另一端连接物理网络。</p>\n<p><br></p>\n<p>综上，以pod0和pod2的通信为例，数据的流转是这样的：</p>\n<p>pod0 –&gt; docker0 –&gt; flannel0 –&gt; flanneld –&gt; Node0物理网卡 –&gt; Node1物理网卡 –&gt; flanneld –&gt; flannel0 –&gt; docker0 –&gt; pod2</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本文主要剖析kubernetes中网络模型，详细介绍pod之间的通信流程。</p>\n<h2 id=\"网络模型概况\"><a href=\"#网络模型概况\" class=\"headerlink\" title=\"网络模型概况\"></a>网络模型概况</h2><h3 id=\"kubernetes对网络的要求\"><a href=\"#kubernetes对网络的要求\" class=\"headerlink\" title=\"kubernetes对网络的要求\"></a>kubernetes对网络的要求</h3><ul>\n<li>所有的容器都可以在不用NAT的方式下同别的容器通信</li>\n<li>所有容器节点都可以在不用NAT的方式下同所有容器通信</li>\n<li>容器的地址和别人看到的地址是同一个地址</li>\n</ul>\n<h3 id=\"kubernetes的网络场景\"><a href=\"#kubernetes的网络场景\" class=\"headerlink\" title=\"kubernetes的网络场景\"></a>kubernetes的网络场景</h3><ul>\n<li><p>容器间直接通信</p>\n<p>同一个pod中，不通容器共享一个网络命名空间，共享同一个Linux协议栈，可以直接通过localhost通信。</p>\n</li>\n<li><p>同一个Node上pod间的通信</p>\n<p>同一个Node内，不同的Pod都有一个全局IP，可以直接通过Pod的IP进行通信。Pod地址和docker0在同一个网段</p>\n</li>\n<li><p>不同Node上的pod间通信</p>\n<p>docker0网桥与宿主机网卡是两个完全不同的IP网段，并且Node之间的通信只能通过宿主机的物理网卡进行，因此要想实现位于不同Node上的Pod容器之间的通信，就必须想办法通过主机的IP地址进行寻址和通信。Kubernetes会记录所有正在运行的Pod的IP分配信息，并且将这些信息保存在etcd中（作为service的Endpoint）。</p>\n<p>因此，实现不同Node上pod之间的通信，需要两个条件</p>\n<ul>\n<li>pod IP做整体规划，整个kubernetes集群内的pod IP不能有冲突，一般通过第三方开源工具管理，如flannel</li>\n<li>将Node IP与该Node内的pod IP关联起来，通过Node IP转发到pod IP</li>\n</ul>\n</li>\n<li><p>pod到service的通信</p>\n<p>pod到service的通信通过kube-proxy实现，底层通过修改iptables规则实现，后文具体介绍。</p>\n</li>\n<li><p>集群外部与内部组件的通信</p>\n<p>通过NodePort或者Ingress实现；</p>\n</li>\n</ul>\n<h3 id=\"kubernetes网络结构\"><a href=\"#kubernetes网络结构\" class=\"headerlink\" title=\"kubernetes网络结构\"></a>kubernetes网络结构</h3><p>为了满足上述要求和场景，kubernetes集群的网络分为三层：</p>\n<ul>\n<li>集群物理网络，即Node节点所属网络；</li>\n<li>Node内部的容器网络，一般是docker0网卡负责管理；</li>\n<li>Node网络和容器网络的连接器，overlay网络，一般flannel负责管理；</li>\n</ul>\n<p>三层网络的结构下图：</p>\n<p><img src=\"Network-Model-in-Kubernetes-6c165903.png\" alt=\"\"></p>\n<p><center>kubernetes网络结构</center><br><br><br><br></p>\n<p>其中，docker0是Node内部不通pod间的默认路由，pod的IP地址从docker0动态分配；flannel负责overylay网络维护，把docker0内的私网和Node节点的网络打通；GateWay则是Node网络中的默认路由，负责物理网络数据包转发。</p>\n<h3 id=\"开源网络组件Flannel\"><a href=\"#开源网络组件Flannel\" class=\"headerlink\" title=\"开源网络组件Flannel\"></a>开源网络组件Flannel</h3><p>在kubernetes中，flannel的左右有以下两点：</p>\n<ul>\n<li>给每个Node上的Docker容器分配不冲突的IP地址</li>\n<li>在这些IP之间建立一个覆盖网络(Overlay Network),通过这个覆盖网络，将数据包原封不动的传递到目标容器内</li>\n</ul>\n<p>首先，flannel创建一个flannel0网桥，一端连接docker0网桥，另一端连接flanneld的服务进程。flanneld进程跟kubernetes集群中的etcd通信，管理网段资源，同时监控每个pod的实际地址，并在内存中建立一个pod节点的路由表。flanneld进程一端连接flannel0网桥，另一端连接物理网络。</p>\n<p><br></p>\n<p>综上，以pod0和pod2的通信为例，数据的流转是这样的：</p>\n<p>pod0 –&gt; docker0 –&gt; flannel0 –&gt; flanneld –&gt; Node0物理网卡 –&gt; Node1物理网卡 –&gt; flanneld –&gt; flannel0 –&gt; docker0 –&gt; pod2</p>\n"},{"title":"DNS Cache In Linux","date":"2018-05-28T11:42:03.000Z","_content":"","source":"_posts/DNS-Cache-In-Linux.md","raw":"---\ntitle: DNS Cache In Linux\ndate: 2018-05-28 19:42:03\ntags:\n---\n","slug":"DNS-Cache-In-Linux","published":1,"updated":"2018-05-28T11:42:03.117Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhsyxci80001a78jcpu9l4at","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Service And Kube-proxy","date":"2018-05-27T15:07:15.000Z","_content":"## Introduction\n本文探究kubernetes架构中，服务发现的具体工作原理，涉及到service定义、服务发现与coredns，kube-proxy工作原理等。\n\n## Service\nkubernetes架构中，Service是对一组提供相同功能pod的抽象。Service通过标签选取服务后端。Service有四种类型：\n  * ClusterIP: 默认类型，为服务自动分配一个虚拟ip，该ip仅集群内部可见\n  * NodePort: 在ClusterIP的基础上，为Service在每台机器上绑定一个端口，外部请求可以通过NodeIP:NodePort访问服务\n  * LoadBalancer: 在NodePort的基础上，通过云服务厂商创建一个外部的负载均衡器\n  * ExternalName: 把服务通过CNAME的方式转发到制定的域名。\n  <br/>\n\n### 创建Service\n  * 一般，在创建服务时，需要指定selector，该selector会和特定label关联，通过和pod的label对比，选择相应的pod作为该服务的endpoints；\n  * 如果不指定selector，可以自己定义endpoint，需要创建一个和Service同名的Endpoints资源，在endpoint中配置指定的ip和端口；\n  * 还有一种情况不需要指定selectors，就是通过DNS CNAME方式把服务转发到指定的域名；\n  <br/>\n\n### headless 服务\nheadless服务是指不需要ClusterIP的服务，在创建服务时指定```spec.clusterIP=None```, 包括两类：\n  * 不指定selectors，但设置externalName；\n  * 不指定selectors，通过A记录设置后段endpoint列表；\n\n## Service Discovery\n服务发现分两种类型：\n* 环境变量\n  * kubelet 会为每个活跃的 Service 添加一组环境变量，要求 Pod 想要访问的任何 Service 必须在 Pod 自己之前被创建\n* DNS\n  * DNS 服务器监视着创建新 Service 的 Kubernetes API，从而为每一个 Service 创建一组 DNS 记录, k8s体系中，DNS服务发现的结构如下图：\n    ![](Service-And-Kube-proxy/Service-And-Kube-proxy-9a3a490b.png)\n\n  如上图所示，kube-dns 或 coredns会watch api-server上Service的变化，并根据规则自动生成DNS A记录。\n\n## Kube-proxy\n用户创建service后，k8s会为这个LB提供一个IP，一般称为cluster IP。 kube-proxy的作用主要是负责service的实现，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。\n\nkube-proxy内部实现原理主要是使用iptables规则，更改filter和nat表。filter表中增加KUBE-FIREWALL和KUBE-SERVICES两个规则链。所有出的报文都经过KUBE-SERVICES，如果没有找到endpoint，则丢弃报文。nat表则设置的规则比较多，主要是各种跳转，下图1中给出了部分。nat表的处理主要步骤为：\n* inbound：在PREROUTING阶段，将所有报文转发到KUBE-SERVICES\n* outbound：在OUTPUT阶段，将所有报文转发到KUBE-SERVICES\n* outbound：在POSTROUTING阶段，将所有报文转发到KUBE-POSTROUTING\n\n以my-nginx服务为例，该服务clusterIP为10.254.82.158，有两个pod，ip分贝为172.30.39.3和172.30.23.8，如下图：\n\n  ![](Service-And-Kube-proxy-02d62faa.png)\n\n使用sudo iptables -t nat -L命令查看nat转发表，只截出了相关的部分，如下图1。\n* 首先pod通过dns查询到clusterIP 10.254.82.158，然后发起调用；\n* 请求进入KUBE-SVC-BEPXDJBUHFCSYIC3\n* KUBE-SVC-BEPXDJBUHFCSYIC3配置了两个pod的概率，0.5\n* 然后请求按照一定概率被转发到另个目的pod，如图2和图3\n\n  ![](Service-And-Kube-proxy-74a4dc02.png)\n  <center>图1</center>\n  </br>\n  </br>\n\n  ![](Service-And-Kube-proxy-90b65a4a.png)\n  <center>图2</center>\n  </br>\n  </br>\n\n  ![](Service-And-Kube-proxy-9bf039d8.png)\n  <center>图3</center>\n  </br>\n  </br>\n\n综上，数据包发到Node的处理过程：\n\n![](Service-And-Kube-proxy-6cc5b0ce.png)\n<center>图片来自http://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html</center>\n\nNode发出的包的处理过程：\n![](Service-And-Kube-proxy-d314f07b.png)\n<center>图片来自http://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html</center>\n\n## CoreDNS\nCoreDNS在k8s中主要负责把Service转换成DNS域名并添加相应的A记录。CoreDNS支持标准的dns协议，插件化，支持多种数据源。\n\n![](Service-And-Kube-proxy-323364fe.png)\n\n## Conclusion\n综上，本文简单介绍了kubernetes体系中，服务发现相关的概念及原理，包括服务类型、服务定义、服务发现方式及kube-proxy的工作原理。\n","source":"_posts/Service-And-Kube-proxy.md","raw":"---\ntitle: Service And Kube-proxy\ndate: 2018-05-27 23:07:15\ntags: service\n---\n## Introduction\n本文探究kubernetes架构中，服务发现的具体工作原理，涉及到service定义、服务发现与coredns，kube-proxy工作原理等。\n\n## Service\nkubernetes架构中，Service是对一组提供相同功能pod的抽象。Service通过标签选取服务后端。Service有四种类型：\n  * ClusterIP: 默认类型，为服务自动分配一个虚拟ip，该ip仅集群内部可见\n  * NodePort: 在ClusterIP的基础上，为Service在每台机器上绑定一个端口，外部请求可以通过NodeIP:NodePort访问服务\n  * LoadBalancer: 在NodePort的基础上，通过云服务厂商创建一个外部的负载均衡器\n  * ExternalName: 把服务通过CNAME的方式转发到制定的域名。\n  <br/>\n\n### 创建Service\n  * 一般，在创建服务时，需要指定selector，该selector会和特定label关联，通过和pod的label对比，选择相应的pod作为该服务的endpoints；\n  * 如果不指定selector，可以自己定义endpoint，需要创建一个和Service同名的Endpoints资源，在endpoint中配置指定的ip和端口；\n  * 还有一种情况不需要指定selectors，就是通过DNS CNAME方式把服务转发到指定的域名；\n  <br/>\n\n### headless 服务\nheadless服务是指不需要ClusterIP的服务，在创建服务时指定```spec.clusterIP=None```, 包括两类：\n  * 不指定selectors，但设置externalName；\n  * 不指定selectors，通过A记录设置后段endpoint列表；\n\n## Service Discovery\n服务发现分两种类型：\n* 环境变量\n  * kubelet 会为每个活跃的 Service 添加一组环境变量，要求 Pod 想要访问的任何 Service 必须在 Pod 自己之前被创建\n* DNS\n  * DNS 服务器监视着创建新 Service 的 Kubernetes API，从而为每一个 Service 创建一组 DNS 记录, k8s体系中，DNS服务发现的结构如下图：\n    ![](Service-And-Kube-proxy/Service-And-Kube-proxy-9a3a490b.png)\n\n  如上图所示，kube-dns 或 coredns会watch api-server上Service的变化，并根据规则自动生成DNS A记录。\n\n## Kube-proxy\n用户创建service后，k8s会为这个LB提供一个IP，一般称为cluster IP。 kube-proxy的作用主要是负责service的实现，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。\n\nkube-proxy内部实现原理主要是使用iptables规则，更改filter和nat表。filter表中增加KUBE-FIREWALL和KUBE-SERVICES两个规则链。所有出的报文都经过KUBE-SERVICES，如果没有找到endpoint，则丢弃报文。nat表则设置的规则比较多，主要是各种跳转，下图1中给出了部分。nat表的处理主要步骤为：\n* inbound：在PREROUTING阶段，将所有报文转发到KUBE-SERVICES\n* outbound：在OUTPUT阶段，将所有报文转发到KUBE-SERVICES\n* outbound：在POSTROUTING阶段，将所有报文转发到KUBE-POSTROUTING\n\n以my-nginx服务为例，该服务clusterIP为10.254.82.158，有两个pod，ip分贝为172.30.39.3和172.30.23.8，如下图：\n\n  ![](Service-And-Kube-proxy-02d62faa.png)\n\n使用sudo iptables -t nat -L命令查看nat转发表，只截出了相关的部分，如下图1。\n* 首先pod通过dns查询到clusterIP 10.254.82.158，然后发起调用；\n* 请求进入KUBE-SVC-BEPXDJBUHFCSYIC3\n* KUBE-SVC-BEPXDJBUHFCSYIC3配置了两个pod的概率，0.5\n* 然后请求按照一定概率被转发到另个目的pod，如图2和图3\n\n  ![](Service-And-Kube-proxy-74a4dc02.png)\n  <center>图1</center>\n  </br>\n  </br>\n\n  ![](Service-And-Kube-proxy-90b65a4a.png)\n  <center>图2</center>\n  </br>\n  </br>\n\n  ![](Service-And-Kube-proxy-9bf039d8.png)\n  <center>图3</center>\n  </br>\n  </br>\n\n综上，数据包发到Node的处理过程：\n\n![](Service-And-Kube-proxy-6cc5b0ce.png)\n<center>图片来自http://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html</center>\n\nNode发出的包的处理过程：\n![](Service-And-Kube-proxy-d314f07b.png)\n<center>图片来自http://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html</center>\n\n## CoreDNS\nCoreDNS在k8s中主要负责把Service转换成DNS域名并添加相应的A记录。CoreDNS支持标准的dns协议，插件化，支持多种数据源。\n\n![](Service-And-Kube-proxy-323364fe.png)\n\n## Conclusion\n综上，本文简单介绍了kubernetes体系中，服务发现相关的概念及原理，包括服务类型、服务定义、服务发现方式及kube-proxy的工作原理。\n","slug":"Service-And-Kube-proxy","published":1,"updated":"2018-05-30T10:28:35.951Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjhsyxcig0004a78ji1ktqf1v","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>本文探究kubernetes架构中，服务发现的具体工作原理，涉及到service定义、服务发现与coredns，kube-proxy工作原理等。</p>\n<h2 id=\"Service\"><a href=\"#Service\" class=\"headerlink\" title=\"Service\"></a>Service</h2><p>kubernetes架构中，Service是对一组提供相同功能pod的抽象。Service通过标签选取服务后端。Service有四种类型：</p>\n<ul>\n<li>ClusterIP: 默认类型，为服务自动分配一个虚拟ip，该ip仅集群内部可见</li>\n<li>NodePort: 在ClusterIP的基础上，为Service在每台机器上绑定一个端口，外部请求可以通过NodeIP:NodePort访问服务</li>\n<li>LoadBalancer: 在NodePort的基础上，通过云服务厂商创建一个外部的负载均衡器</li>\n<li>ExternalName: 把服务通过CNAME的方式转发到制定的域名。<br><br></li>\n</ul>\n<h3 id=\"创建Service\"><a href=\"#创建Service\" class=\"headerlink\" title=\"创建Service\"></a>创建Service</h3><ul>\n<li>一般，在创建服务时，需要指定selector，该selector会和特定label关联，通过和pod的label对比，选择相应的pod作为该服务的endpoints；</li>\n<li>如果不指定selector，可以自己定义endpoint，需要创建一个和Service同名的Endpoints资源，在endpoint中配置指定的ip和端口；</li>\n<li>还有一种情况不需要指定selectors，就是通过DNS CNAME方式把服务转发到指定的域名；<br><br></li>\n</ul>\n<h3 id=\"headless-服务\"><a href=\"#headless-服务\" class=\"headerlink\" title=\"headless 服务\"></a>headless 服务</h3><p>headless服务是指不需要ClusterIP的服务，在创建服务时指定<code>spec.clusterIP=None</code>, 包括两类：</p>\n<ul>\n<li>不指定selectors，但设置externalName；</li>\n<li>不指定selectors，通过A记录设置后段endpoint列表；</li>\n</ul>\n<h2 id=\"Service-Discovery\"><a href=\"#Service-Discovery\" class=\"headerlink\" title=\"Service Discovery\"></a>Service Discovery</h2><p>服务发现分两种类型：</p>\n<ul>\n<li>环境变量<ul>\n<li>kubelet 会为每个活跃的 Service 添加一组环境变量，要求 Pod 想要访问的任何 Service 必须在 Pod 自己之前被创建</li>\n</ul>\n</li>\n<li><p>DNS</p>\n<ul>\n<li>DNS 服务器监视着创建新 Service 的 Kubernetes API，从而为每一个 Service 创建一组 DNS 记录, k8s体系中，DNS服务发现的结构如下图：<br><img src=\"Service-And-Kube-proxy/Service-And-Kube-proxy-9a3a490b.png\" alt=\"\"></li>\n</ul>\n<p>如上图所示，kube-dns 或 coredns会watch api-server上Service的变化，并根据规则自动生成DNS A记录。</p>\n</li>\n</ul>\n<h2 id=\"Kube-proxy\"><a href=\"#Kube-proxy\" class=\"headerlink\" title=\"Kube-proxy\"></a>Kube-proxy</h2><p>用户创建service后，k8s会为这个LB提供一个IP，一般称为cluster IP。 kube-proxy的作用主要是负责service的实现，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。</p>\n<p>kube-proxy内部实现原理主要是使用iptables规则，更改filter和nat表。filter表中增加KUBE-FIREWALL和KUBE-SERVICES两个规则链。所有出的报文都经过KUBE-SERVICES，如果没有找到endpoint，则丢弃报文。nat表则设置的规则比较多，主要是各种跳转，下图1中给出了部分。nat表的处理主要步骤为：</p>\n<ul>\n<li>inbound：在PREROUTING阶段，将所有报文转发到KUBE-SERVICES</li>\n<li>outbound：在OUTPUT阶段，将所有报文转发到KUBE-SERVICES</li>\n<li>outbound：在POSTROUTING阶段，将所有报文转发到KUBE-POSTROUTING</li>\n</ul>\n<p>以my-nginx服务为例，该服务clusterIP为10.254.82.158，有两个pod，ip分贝为172.30.39.3和172.30.23.8，如下图：</p>\n<p>  <img src=\"Service-And-Kube-proxy-02d62faa.png\" alt=\"\"></p>\n<p>使用sudo iptables -t nat -L命令查看nat转发表，只截出了相关的部分，如下图1。</p>\n<ul>\n<li>首先pod通过dns查询到clusterIP 10.254.82.158，然后发起调用；</li>\n<li>请求进入KUBE-SVC-BEPXDJBUHFCSYIC3</li>\n<li>KUBE-SVC-BEPXDJBUHFCSYIC3配置了两个pod的概率，0.5</li>\n<li><p>然后请求按照一定概率被转发到另个目的pod，如图2和图3</p>\n<p><img src=\"Service-And-Kube-proxy-74a4dc02.png\" alt=\"\"><br><center>图1</center><br><br><br><br></p>\n<p><img src=\"Service-And-Kube-proxy-90b65a4a.png\" alt=\"\"><br><center>图2</center><br><br><br><br></p>\n<p><img src=\"Service-And-Kube-proxy-9bf039d8.png\" alt=\"\"><br><center>图3</center><br><br><br><br></p>\n</li>\n</ul>\n<p>综上，数据包发到Node的处理过程：</p>\n<p><img src=\"Service-And-Kube-proxy-6cc5b0ce.png\" alt=\"\"></p>\n<center>图片来自<a href=\"http://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html\" target=\"_blank\" rel=\"noopener\">http://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html</a></center>\n\n<p>Node发出的包的处理过程：<br><img src=\"Service-And-Kube-proxy-d314f07b.png\" alt=\"\"></p>\n<center>图片来自<a href=\"http://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html\" target=\"_blank\" rel=\"noopener\">http://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html</a></center>\n\n<h2 id=\"CoreDNS\"><a href=\"#CoreDNS\" class=\"headerlink\" title=\"CoreDNS\"></a>CoreDNS</h2><p>CoreDNS在k8s中主要负责把Service转换成DNS域名并添加相应的A记录。CoreDNS支持标准的dns协议，插件化，支持多种数据源。</p>\n<p><img src=\"Service-And-Kube-proxy-323364fe.png\" alt=\"\"></p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>综上，本文简单介绍了kubernetes体系中，服务发现相关的概念及原理，包括服务类型、服务定义、服务发现方式及kube-proxy的工作原理。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>本文探究kubernetes架构中，服务发现的具体工作原理，涉及到service定义、服务发现与coredns，kube-proxy工作原理等。</p>\n<h2 id=\"Service\"><a href=\"#Service\" class=\"headerlink\" title=\"Service\"></a>Service</h2><p>kubernetes架构中，Service是对一组提供相同功能pod的抽象。Service通过标签选取服务后端。Service有四种类型：</p>\n<ul>\n<li>ClusterIP: 默认类型，为服务自动分配一个虚拟ip，该ip仅集群内部可见</li>\n<li>NodePort: 在ClusterIP的基础上，为Service在每台机器上绑定一个端口，外部请求可以通过NodeIP:NodePort访问服务</li>\n<li>LoadBalancer: 在NodePort的基础上，通过云服务厂商创建一个外部的负载均衡器</li>\n<li>ExternalName: 把服务通过CNAME的方式转发到制定的域名。<br><br></li>\n</ul>\n<h3 id=\"创建Service\"><a href=\"#创建Service\" class=\"headerlink\" title=\"创建Service\"></a>创建Service</h3><ul>\n<li>一般，在创建服务时，需要指定selector，该selector会和特定label关联，通过和pod的label对比，选择相应的pod作为该服务的endpoints；</li>\n<li>如果不指定selector，可以自己定义endpoint，需要创建一个和Service同名的Endpoints资源，在endpoint中配置指定的ip和端口；</li>\n<li>还有一种情况不需要指定selectors，就是通过DNS CNAME方式把服务转发到指定的域名；<br><br></li>\n</ul>\n<h3 id=\"headless-服务\"><a href=\"#headless-服务\" class=\"headerlink\" title=\"headless 服务\"></a>headless 服务</h3><p>headless服务是指不需要ClusterIP的服务，在创建服务时指定<code>spec.clusterIP=None</code>, 包括两类：</p>\n<ul>\n<li>不指定selectors，但设置externalName；</li>\n<li>不指定selectors，通过A记录设置后段endpoint列表；</li>\n</ul>\n<h2 id=\"Service-Discovery\"><a href=\"#Service-Discovery\" class=\"headerlink\" title=\"Service Discovery\"></a>Service Discovery</h2><p>服务发现分两种类型：</p>\n<ul>\n<li>环境变量<ul>\n<li>kubelet 会为每个活跃的 Service 添加一组环境变量，要求 Pod 想要访问的任何 Service 必须在 Pod 自己之前被创建</li>\n</ul>\n</li>\n<li><p>DNS</p>\n<ul>\n<li>DNS 服务器监视着创建新 Service 的 Kubernetes API，从而为每一个 Service 创建一组 DNS 记录, k8s体系中，DNS服务发现的结构如下图：<br><img src=\"Service-And-Kube-proxy/Service-And-Kube-proxy-9a3a490b.png\" alt=\"\"></li>\n</ul>\n<p>如上图所示，kube-dns 或 coredns会watch api-server上Service的变化，并根据规则自动生成DNS A记录。</p>\n</li>\n</ul>\n<h2 id=\"Kube-proxy\"><a href=\"#Kube-proxy\" class=\"headerlink\" title=\"Kube-proxy\"></a>Kube-proxy</h2><p>用户创建service后，k8s会为这个LB提供一个IP，一般称为cluster IP。 kube-proxy的作用主要是负责service的实现，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。</p>\n<p>kube-proxy内部实现原理主要是使用iptables规则，更改filter和nat表。filter表中增加KUBE-FIREWALL和KUBE-SERVICES两个规则链。所有出的报文都经过KUBE-SERVICES，如果没有找到endpoint，则丢弃报文。nat表则设置的规则比较多，主要是各种跳转，下图1中给出了部分。nat表的处理主要步骤为：</p>\n<ul>\n<li>inbound：在PREROUTING阶段，将所有报文转发到KUBE-SERVICES</li>\n<li>outbound：在OUTPUT阶段，将所有报文转发到KUBE-SERVICES</li>\n<li>outbound：在POSTROUTING阶段，将所有报文转发到KUBE-POSTROUTING</li>\n</ul>\n<p>以my-nginx服务为例，该服务clusterIP为10.254.82.158，有两个pod，ip分贝为172.30.39.3和172.30.23.8，如下图：</p>\n<p>  <img src=\"Service-And-Kube-proxy-02d62faa.png\" alt=\"\"></p>\n<p>使用sudo iptables -t nat -L命令查看nat转发表，只截出了相关的部分，如下图1。</p>\n<ul>\n<li>首先pod通过dns查询到clusterIP 10.254.82.158，然后发起调用；</li>\n<li>请求进入KUBE-SVC-BEPXDJBUHFCSYIC3</li>\n<li>KUBE-SVC-BEPXDJBUHFCSYIC3配置了两个pod的概率，0.5</li>\n<li><p>然后请求按照一定概率被转发到另个目的pod，如图2和图3</p>\n<p><img src=\"Service-And-Kube-proxy-74a4dc02.png\" alt=\"\"><br><center>图1</center><br><br><br><br></p>\n<p><img src=\"Service-And-Kube-proxy-90b65a4a.png\" alt=\"\"><br><center>图2</center><br><br><br><br></p>\n<p><img src=\"Service-And-Kube-proxy-9bf039d8.png\" alt=\"\"><br><center>图3</center><br><br><br><br></p>\n</li>\n</ul>\n<p>综上，数据包发到Node的处理过程：</p>\n<p><img src=\"Service-And-Kube-proxy-6cc5b0ce.png\" alt=\"\"></p>\n<center>图片来自<a href=\"http://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html\" target=\"_blank\" rel=\"noopener\">http://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html</a></center>\n\n<p>Node发出的包的处理过程：<br><img src=\"Service-And-Kube-proxy-d314f07b.png\" alt=\"\"></p>\n<center>图片来自<a href=\"http://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html\" target=\"_blank\" rel=\"noopener\">http://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html</a></center>\n\n<h2 id=\"CoreDNS\"><a href=\"#CoreDNS\" class=\"headerlink\" title=\"CoreDNS\"></a>CoreDNS</h2><p>CoreDNS在k8s中主要负责把Service转换成DNS域名并添加相应的A记录。CoreDNS支持标准的dns协议，插件化，支持多种数据源。</p>\n<p><img src=\"Service-And-Kube-proxy-323364fe.png\" alt=\"\"></p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>综上，本文简单介绍了kubernetes体系中，服务发现相关的概念及原理，包括服务类型、服务定义、服务发现方式及kube-proxy的工作原理。</p>\n"}],"PostAsset":[{"_id":"source/_posts/Network-Model-in-Kubernetes/Network-Model-in-Kubernetes-6c165903.png","post":"cjhsyxci40000a78jrnbsbq6l","slug":"Network-Model-in-Kubernetes-6c165903.png","modified":1,"renderable":1},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-02d62faa.png","post":"cjhsyxcig0004a78ji1ktqf1v","slug":"Service-And-Kube-proxy-02d62faa.png","modified":1,"renderable":1},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-323364fe.png","post":"cjhsyxcig0004a78ji1ktqf1v","slug":"Service-And-Kube-proxy-323364fe.png","modified":1,"renderable":1},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-4afab240.png","post":"cjhsyxcig0004a78ji1ktqf1v","slug":"Service-And-Kube-proxy-4afab240.png","modified":1,"renderable":1},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-6cc5b0ce.png","post":"cjhsyxcig0004a78ji1ktqf1v","slug":"Service-And-Kube-proxy-6cc5b0ce.png","modified":1,"renderable":1},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-74a4dc02.png","post":"cjhsyxcig0004a78ji1ktqf1v","slug":"Service-And-Kube-proxy-74a4dc02.png","modified":1,"renderable":1},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-90b65a4a.png","post":"cjhsyxcig0004a78ji1ktqf1v","slug":"Service-And-Kube-proxy-90b65a4a.png","modified":1,"renderable":1},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-9a3a490b.png","post":"cjhsyxcig0004a78ji1ktqf1v","slug":"Service-And-Kube-proxy-9a3a490b.png","modified":1,"renderable":1},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-9bf039d8.png","post":"cjhsyxcig0004a78ji1ktqf1v","slug":"Service-And-Kube-proxy-9bf039d8.png","modified":1,"renderable":1},{"_id":"source/_posts/Service-And-Kube-proxy/Service-And-Kube-proxy-d314f07b.png","post":"cjhsyxcig0004a78ji1ktqf1v","slug":"Service-And-Kube-proxy-d314f07b.png","modified":1,"renderable":1}],"PostCategory":[],"PostTag":[{"post_id":"cjhsyxci40000a78jrnbsbq6l","tag_id":"cjhsyxcib0002a78jr61e03ua","_id":"cjhsyxcie0003a78jvygra9bs"},{"post_id":"cjhsyxcig0004a78ji1ktqf1v","tag_id":"cjhsyxcih0005a78jw0yzbbm6","_id":"cjhsyxcih0006a78jly6jf2xf"}],"Tag":[{"name":"kubernetes Network","_id":"cjhsyxcib0002a78jr61e03ua"},{"name":"service","_id":"cjhsyxcih0005a78jw0yzbbm6"}]}}